{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils_data\n",
    "\n",
    "import time as time\n",
    "\n",
    "from train_deep import run_epoch, train, infer_model\n",
    "from sota_models.deeplob import DeepLOB\n",
    "from sota_models.deepreslob import DeepResLOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Weights & Biases!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\asang/.netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login 031af70dba88e746696d15cc5bdddf1dc268ab62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, dataset, k, token_name=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        model - the neural network model, should be either DeepLOB or DeepResLOB\n",
    "        dataset - \"FI-2010\" for the benchmark dataset;\n",
    "                  \"Tokens-setup1\" for the first experimental setup for tokens;\n",
    "                  \"Tokens-setup2\" for the second experimental setup for tokens;\n",
    "        k - desired prediction horizon, available:\n",
    "            FI-2010 : k = 1, 5, 10;\n",
    "            Crypto : k = 1, 5, 10, 20\n",
    "        token_name - is only used in case \"Tokens-setup1\" is used, viable values:\n",
    "                     \"BTC\", \"ETH\", \"LTC\", \"XRP\"\n",
    "    \"\"\"\n",
    "    model_name = model.__class__.__name__\n",
    "    path = './weights/'+model_name+'/{}/'.format(dataset)\n",
    "    if dataset == 'Tokens-setup1':\n",
    "        path += '/{}/'.format(token_name.lower())\n",
    "    if model_name == 'DeepResLOB':\n",
    "        model_name = 'ResLOB'\n",
    "    path += model_name.lower()+'_'\n",
    "    if dataset == 'Tokens-setup1':\n",
    "        path += token_name.lower()+'_'\n",
    "    path += str(k)\n",
    "    model.load_state_dict(torch.load(path+'.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI-2010 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Project Data\")\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\"./FI-2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_X, train_Y = np.load(AUX_DATA_ROOT / 'train_data.npy'), np.load(AUX_DATA_ROOT / 'train_labels.npy')\n",
    "val_X, val_Y = np.load(AUX_DATA_ROOT / 'val_data.npy'), np.load(AUX_DATA_ROOT / 'val_labels.npy')\n",
    "test_X, test_Y = np.load(AUX_DATA_ROOT / 'test_data.npy'), np.load(AUX_DATA_ROOT / 'test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_wrapper import FI_Dataset\n",
    "\n",
    "# wrap data into PyTorch datasets\n",
    "k = 0 # prediction horizon: 0 := (k = 1), 1 := (k = 2), 2 := (k = 3), 3 := (k = 5), 4 := (k = 10)\n",
    "T = 100 # sliding time window size\n",
    "train_dset, val_dset, test_dset = FI_Dataset(train_X, train_Y, T, k), FI_Dataset(val_X, val_Y, T, k),\\\n",
    "                                  FI_Dataset(test_X, test_Y, T, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    train_loader = utils_data.DataLoader(train_dset, shuffle=True, num_workers=4, batch_size=64, pin_memory=True)\n",
    "    val_loader   = utils_data.DataLoader(val_dset, shuffle=False, num_workers=4, batch_size=64, pin_memory=True)\n",
    "    \n",
    "    # DeepLOB\n",
    "    deep_lob = DeepLOB().to(device)\n",
    "    dl_optimizer = optim.Adam(deep_lob.parameters(), lr=0.01, eps=1)\n",
    "    \n",
    "    # DeepResLOB\n",
    "    deep_res_lob = DeepResLOB().to(device)\n",
    "    rl_optimizer = optim.Adam(deep_res_lob.parameters(), lr=0.01, eps=1, weight_decay=1e-4) # 1e-3 for k = 10\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train(deep_lob, dl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n",
    "          ch_name='./deeplob_best.pth', early_stopping=True, es_patience=20, metric='accuracy', freq=1)\n",
    "    train(deep_res_lob, rl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n",
    "          ch_name='./reslob_best.pth', early_stopping=True, es_patience=20, metric='accuracy', freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = utils_data.DataLoader(test_dset, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\asang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(81.79593373493977, 80.88149782728132, 83.02023544848606, 81.79593373493977)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_lob = DeepLOB()\n",
    "load_weights(deep_lob, 'FI-2010', 1)\n",
    "deep_lob = deep_lob.to(device)\n",
    "\n",
    "infer_model(deep_lob, test_loader, device) # (f1, acc, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./weights/DeepResLOB/FI-2010/reslob_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(82.43913152610442, 81.28611724970958, 83.9808184022695, 82.43913152610442)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_res_lob = DeepResLOB()\n",
    "load_weights(deep_res_lob, 'FI-2010', 1)\n",
    "deep_res_lob = deep_res_lob.to(device)\n",
    "\n",
    "infer_model(deep_res_lob, test_loader, device) # (f1, acc, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency - setup 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Processed_datasets\")\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\"./crypto_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_token(token_name, k):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        token_name - available tokens: \"BTC\", \"ETH\", \"LTC\", \"XRP\",\n",
    "        k - prediction horizon, available: 1, 5, 10, 20\n",
    "    \"\"\"\n",
    "    data = np.load(AUX_DATA_ROOT / ('input_'+token_name.upper()+'USDT'+'_'+str(k)+'.npy'))\n",
    "    labels = np.load(AUX_DATA_ROOT / ('labels_'+token_name.upper()+'USDT'+'_'+str(k)+'.npy'))\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_token(\"BTC\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_wrapper import CryptoDataset\n",
    "\n",
    "def train_val_test_split(data, labels, splits=None, T=60):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        data, labels - input data and labels,\n",
    "        splits - tuple of integers (train_size, val_size),\n",
    "        T - sliding time window size\n",
    "    \"\"\"\n",
    "    if splits is None:\n",
    "        splits = (int(0.7 * data.shape[0]), int(0.15 * data.shape[0]))\n",
    "    train_X, train_Y = data[:splits[0]], labels[:splits[0]]\n",
    "    val_X, val_Y = data[splits[0]:splits[0]+splits[1]], labels[splits[0]:splits[0]+splits[1]]\n",
    "    test_X, test_Y = data[splits[0]+splits[1]:], labels[splits[0]+splits[1]:]\n",
    "    \n",
    "    num_labels = [(train_Y == i).sum() for i in range(3)]\n",
    "    max_labels = max(num_labels)\n",
    "    weights    = [max_labels / num_labels[i] for i in range(3)]\n",
    "    \n",
    "    train_dset = CryptoDataset(train_X, train_Y, T)\n",
    "    val_dset = CryptoDataset(val_X, val_Y, T)\n",
    "    test_dset = CryptoDataset(test_X, test_Y, T)\n",
    "    \n",
    "    return train_dset, val_dset, test_dset, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, val_dset, test_dset, weights = train_val_test_split(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    train_loader = utils_data.DataLoader(train_dset, shuffle=True, num_workers=4, batch_size=64, pin_memory=True)\n",
    "    val_loader   = utils_data.DataLoader(val_dset, shuffle=False, num_workers=4, batch_size=64, pin_memory=True)\n",
    "    \n",
    "    # DeepLOB\n",
    "    deep_lob = DeepLOB().to(device)\n",
    "    dl_optimizer = optim.Adam(deep_lob.parameters(), lr=0.01, eps=1)\n",
    "    \n",
    "    # DeepResLOB\n",
    "    deep_res_lob = DeepResLOB().to(device)\n",
    "    rl_optimizer = optim.Adam(deep_res_lob.parameters(), lr=0.01, eps=1, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(torch.tensor(weights, dtype=torch.float, device=device))\n",
    "    \n",
    "    train(deep_lob, dl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n",
    "          ch_name='./deeplob_crypto_best.pth', early_stopping=True, es_patience=20, metric='f1 score', freq=1)\n",
    "    train(deep_res_lob, rl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n",
    "          ch_name='./reslob_crypto_best.pth', early_stopping=True, es_patience=20, metric='f1 score', freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = utils_data.DataLoader(test_dset, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81.0210905447909, 81.88823049930238, 89.20778206023384, 81.02109053497942)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_lob = DeepLOB()\n",
    "load_weights(deep_lob, 'Tokens-setup1', 1, 'BTC')\n",
    "deep_lob = deep_lob.to(device)\n",
    "\n",
    "infer_model(deep_lob, test_loader, device) # (f1, acc, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84.83539094895492, 84.32400780474309, 89.11055882230482, 84.83539094650206)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_res_lob = DeepResLOB()\n",
    "load_weights(deep_res_lob, 'Tokens-setup1', 1, 'BTC')\n",
    "deep_res_lob = deep_res_lob.to(device)\n",
    "\n",
    "infer_model(deep_res_lob, test_loader, device) # (f1, acc, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency - setup 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Processed_datasets\")\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\"./crypto_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(k, splits=None, T=60):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        k - prediction horizon, available: 1, 5, 10, 20,\n",
    "        splits - tuple of two ints (train_size, val_size),\n",
    "        T - sliding time window size\n",
    "    \"\"\"\n",
    "    btc_data = np.load(AUX_DATA_ROOT / ('input_BTCUSDT_'+str(k)+'.npy'))\n",
    "    btc_labels = np.load(AUX_DATA_ROOT / ('labels_BTCUSDT_'+str(k)+'.npy'))\n",
    "    \n",
    "    ltc_data = np.load(AUX_DATA_ROOT / ('input_LTCUSDT_'+str(k)+'.npy'))\n",
    "    ltc_labels = np.load(AUX_DATA_ROOT / ('labels_LTCUSDT_'+str(k)+'.npy'))\n",
    "    \n",
    "    eth_data = np.load(AUX_DATA_ROOT / ('input_ETHUSDT_'+str(k)+'.npy'))\n",
    "    eth_labels = np.load(AUX_DATA_ROOT / ('labels_ETHUSDT_'+str(k)+'.npy'))\n",
    "    \n",
    "    if splits is None:\n",
    "        splits = [(int(0.8 * btc_data.shape[0]), int(0.1 * btc_data.shape[0])),\\\n",
    "                  (int(0.8 * eth_data.shape[0]), int(0.1 * eth_data.shape[0]))]\n",
    "    \n",
    "    # Train and validation sets\n",
    "    train_X = np.concatenate((btc_data[:splits[0][0]], ltc_data[:splits[0][0]], eth_data[:splits[1][0]]), axis=0)\n",
    "    train_Y = np.concatenate((btc_labels[:splits[0][0]], ltc_labels[:splits[0][0]], eth_labels[:splits[1][0]]), axis=0)\n",
    "\n",
    "    val_X   = np.concatenate((btc_data[splits[0][0]:splits[0][0]+splits[0][1]],\\\n",
    "                              ltc_data[splits[0][0]:splits[0][0]+splits[0][1]],\\\n",
    "                              eth_data[splits[1][0]:splits[1][0]+splits[1][1]]), axis=0)\n",
    "    val_Y   = np.concatenate((btc_labels[splits[0][0]:splits[0][0]+splits[0][1]],\\\n",
    "                              ltc_labels[splits[0][0]:splits[0][0]+splits[0][1]],\\\n",
    "                              eth_labels[splits[1][0]:splits[1][0]+splits[1][1]]), axis=0)\n",
    "    \n",
    "    num_labels = [(train_Y == i).sum() for i in range(3)]\n",
    "    max_labels = max(num_labels)\n",
    "    weights    = [max_labels / num_labels[i] for i in range(3)]\n",
    "    \n",
    "    train_dset = CryptoDataset(train_X, train_Y, T)\n",
    "    val_dset = CryptoDataset(val_X, val_Y, T)\n",
    "    \n",
    "    # Separate test sets\n",
    "    test_btc_X, test_btc_Y = btc_data[splits[0][0]+splits[0][1]:], btc_labels[splits[0][0]+splits[0][1]:]\n",
    "    test_ltc_X, test_ltc_Y = ltc_data[splits[0][0]+splits[0][1]:], ltc_labels[splits[0][0]+splits[0][1]:]\n",
    "    test_eth_X, test_eth_Y = eth_data[splits[1][0]+splits[1][1]:], eth_labels[splits[1][0]+splits[1][1]:]\n",
    "    \n",
    "    test_btc_dset = CryptoDataset(test_btc_X, test_btc_Y, T)\n",
    "    test_ltc_dset = CryptoDataset(test_ltc_X, test_ltc_Y, T)\n",
    "    test_eth_dset = CryptoDataset(test_eth_X, test_eth_Y, T)\n",
    "    \n",
    "    test_sets = [test_btc_dset, test_ltc_dset, test_eth_dset]\n",
    "    \n",
    "    return train_dset, val_dset, test_sets, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, val_dset, test_sets, weights = load_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/asang/crypto_results\" target=\"_blank\">https://app.wandb.ai/asang/crypto_results</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/asang/crypto_results/runs/1d8e98rl\" target=\"_blank\">https://app.wandb.ai/asang/crypto_results/runs/1d8e98rl</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.9.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-0da1008feaa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     train(deep_lob, dl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n\u001b[1;32m---> 15\u001b[1;33m           ch_name='./deeplob_crypto_best.pth', early_stopping=True, es_patience=20, metric='f1 score', freq=1)\n\u001b[0m\u001b[0;32m     16\u001b[0m     train(deep_res_lob, rl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n\u001b[0;32m     17\u001b[0m           ch_name='./reslob_crypto_best.pth', early_stopping=True, es_patience=20, metric='f1 score', freq=1)\n",
      "\u001b[1;32m~\\Documents\\Skoltech\\Deep Learning\\Project\\git\\train_deep.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, train_loader, val_loader, device, n_epochs, metric, project_name, scheduler, checkpoint, ch_name, early_stopping, es_patience, freq, verbose)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_m\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mtrain_val_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Skoltech\\Deep Learning\\Project\\git\\train_deep.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(model, optimizer, criterion, dataloader, device, epoch, metric, mode)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mepoch_metric\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcalculate_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if DO_TRAIN:\n",
    "    train_loader = utils_data.DataLoader(train_dset, shuffle=True, num_workers=4, batch_size=64, pin_memory=True)\n",
    "    val_loader   = utils_data.DataLoader(val_dset, shuffle=False, num_workers=4, batch_size=64, pin_memory=True)\n",
    "    \n",
    "    # DeepLOB\n",
    "    deep_lob = DeepLOB().to(device)\n",
    "    dl_optimizer = optim.Adam(deep_lob.parameters(), lr=0.01, eps=1)\n",
    "    \n",
    "    # DeepResLOB\n",
    "    deep_res_lob = DeepResLOB().to(device)\n",
    "    rl_optimizer = optim.Adam(deep_res_lob.parameters(), lr=0.01, eps=1, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(torch.tensor(weights, dtype=torch.float, device=device))\n",
    "    \n",
    "    train(deep_lob, dl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n",
    "          ch_name='./deeplob_crypto_best.pth', early_stopping=True, es_patience=20, metric='f1 score', freq=1)\n",
    "    train(deep_res_lob, rl_optimizer, criterion, train_loader, val_loader, device, n_epochs=100,\\\n",
    "          ch_name='./reslob_crypto_best.pth', early_stopping=True, es_patience=20, metric='f1 score', freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup2(model, test_sets, device, k=1):\n",
    "    names = [\"BTC\", \"LTC\", \"ETH\"]\n",
    "\n",
    "    for i in range(3):\n",
    "        test_loader = utils_data.DataLoader(test_sets[i], shuffle=False, batch_size=64)\n",
    "\n",
    "        test_acc, test_f1, test_prec, test_rec = infer_model(model, test_loader, device)\n",
    "        print(names[i]+\":\")\n",
    "        print(\"Test accuracy: {} %\".format(test_acc))\n",
    "        print(\"Test F1 score: {} %\".format(test_f1))\n",
    "        print(\"Test precision score: {} %\".format(test_prec))\n",
    "        print(\"Test recall score: {} %\".format(test_rec))\n",
    "    \n",
    "    test_X, test_Y = load_token(\"XRP\", k)\n",
    "    test_dset   = CryptoDataset(test_X, test_Y, T=60)\n",
    "    test_loader = utils_data.DataLoader(test_dset, shuffle=False, batch_size=64)\n",
    "    \n",
    "    test_acc, test_f1, test_prec, test_rec = infer_model(model, test_loader, device)\n",
    "    print(\"XRP transfer learning:\")\n",
    "    print(\"Test accuracy: {} %\".format(test_acc))\n",
    "    print(\"Test F1 score: {} %\".format(test_f1))\n",
    "    print(\"Test precision score: {} %\".format(test_prec))\n",
    "    print(\"Test recall score: {} %\".format(test_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC:\n",
      "Test accuracy: 86.69367285422337 %\n",
      "Test F1 score: 87.43848472278508 %\n",
      "Test precision score: 91.66555230522587 %\n",
      "Test recall score: 86.69367283950618 %\n",
      "LTC:\n",
      "Test accuracy: 85.5150463036549 %\n",
      "Test F1 score: 85.60711868002161 %\n",
      "Test precision score: 90.3459292213265 %\n",
      "Test recall score: 85.51504629629629 %\n",
      "ETH:\n",
      "Test accuracy: 88.4331983590276 %\n",
      "Test F1 score: 88.41068679439317 %\n",
      "Test precision score: 91.25881901563436 %\n",
      "Test recall score: 88.43319835053993 %\n",
      "XRP transfer learning:\n",
      "Test accuracy: 83.91782407407408 %\n",
      "Test F1 score: 84.21731384536011 %\n",
      "Test precision score: 90.04753871511154 %\n",
      "Test recall score: 83.91782407407408 %\n"
     ]
    }
   ],
   "source": [
    "deep_lob = DeepLOB()\n",
    "load_weights(deep_lob, 'Tokens-setup2', 1)\n",
    "deep_lob = deep_lob.to(device)\n",
    "\n",
    "setup2(deep_lob, test_sets, device, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC:\n",
      "Test accuracy: 89.90354939007464 %\n",
      "Test F1 score: 90.40081105959054 %\n",
      "Test precision score: 93.143718531075 %\n",
      "Test recall score: 89.90354938271605 %\n",
      "LTC:\n",
      "Test accuracy: 85.5054012419265 %\n",
      "Test F1 score: 85.62350985419458 %\n",
      "Test precision score: 90.22510157121391 %\n",
      "Test recall score: 85.5054012345679 %\n",
      "ETH:\n",
      "Test accuracy: 88.72189094435494 %\n",
      "Test F1 score: 88.66901986114077 %\n",
      "Test precision score: 91.55992096511478 %\n",
      "Test recall score: 88.72189094576956 %\n",
      "XRP transfer learning:\n",
      "Test accuracy: 86.19695216049382 %\n",
      "Test F1 score: 86.49951240356641 %\n",
      "Test precision score: 91.20760804953395 %\n",
      "Test recall score: 86.19695216049382 %\n"
     ]
    }
   ],
   "source": [
    "deep_res_lob = DeepResLOB()\n",
    "load_weights(deep_res_lob, 'Tokens-setup2', 1)\n",
    "deep_lob = deep_res_lob.to(device)\n",
    "\n",
    "setup2(deep_res_lob, test_sets, device, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
